In this chapter, the author explains how reinforcement learning is used to train artificial intelligence agents. He uses examples from games such as Super Mario Bros. and chess to demonstrate how an agent learns by interacting with its environment and receiving rewards. From this interaction, the agent learns how to perform actions in a game, gain experience, and then move to the next state. The agent always seeks to maximize the expected return, which is the sum of all the actions taken.
	Now that we've learned about rewards, it's time to get down to business. The reward is "the only feedback" the agent gets in RL--it's the only way for him to know if he/she got rewarded for taking a certain action. In order to make this process as smooth as possible, the authors divide each step into two parts: the episodic tasks and the continuous tasks. Episodic tasks are tasks that have a starting place and an end point and can begin and end with the same state of the game. For example, an agent executing a stock trading task, there's no starting spot and no end point; the agent just runs until they decide to stop him/her. Continuous tasks have no such starting and end points and can be divided into two types: exploration and exploitation. Exploration is simply trying to explore the environment to find out about it; exploitation is exploiting it for its information value. In other words, you might go to a good restaurant once in a while and then never visit it again, but that would be a huge mistake. With that out of the way, let's talk about the principle of reinforcement learning. What is reinforcement learning, you may be asking? Well, to answer your question, we have two approaches to training an agent to learn how to react to the environment. First, we use stochastic methods--i.e., we learn a function called a policy-basis, which tells us what actions to take based on the expected value of being at the current state. We also use reinforcement learning algorithms like deep neural networks to do exactly the same thing. Deep Learning even uses machine learning to solve problems like this one.
	The course syllabus for coders explains how reinforcement learning is used to learn from actions. An agent learns by interacting with its environment and receiving rewards, either negative or positive, as feedback. The goal of all RL agents is to maximize the expected cumulative return, also called the expected payoff. In order to do this, you need to find an optimal policy, which is the brain of your AI. The optimal policy will be the one that gives you the most expected return. You can achieve this by training your policy using either policy-based methods or by using a value function to predict the expected return at each state. We'll talk more about both types of methods in the next chapter. Now that you've learned all there is to know about reinforcement learning, ipynb is happy to share his work with the community on GitHub.